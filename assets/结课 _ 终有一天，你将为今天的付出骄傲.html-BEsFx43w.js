import{_ as t}from"./plugin-vue_export-helper-DlAUqK2U.js";import{c as a,a as o,o as r}from"./app-d8EKP-K0.js";const p={};function n(i,e){return r(),a("div",null,e[0]||(e[0]=[o('<h1 id="结课-终有一天-你将为今天的付出骄傲" tabindex="-1"><a class="header-anchor" href="#结课-终有一天-你将为今天的付出骄傲"><span>结课 _ 终有一天，你将为今天的付出骄傲</span></a></h1><p><audio id="audio" title="结课 | 终有一天，你将为今天的付出骄傲" controls="" preload="none"><source id="mp3" src="https://static001.geekbang.org/resource/audio/6f/46/6f79a1684d6e01a56f79d89345d95c46.mp3"></audio></p><p>不知不觉间，又一个40期的机器学习专栏也走到了尾声。在专栏里，我从理解概率的两大流派入手，以每种流派中的各个模型为主线，对统计机器学习和贝叶斯机器学习做了系统的介绍，并从这些模型中梳理出它们之间关系的脉络，帮助你尽可能地从更加宏观的角度来理解模型内部的关联。</p><h2 id="内容-由博返约求精深" tabindex="-1"><a class="header-anchor" href="#内容-由博返约求精深"><span>内容：由博返约求精深</span></a></h2><p>和上一季的“人工智能基础课”相比，这一季专栏的内容聚焦于机器学习一点，力求更加深入地挖掘这个主题。增加深度意味着提升难度，无论是写作的我还是阅读的你，都需要投入更多的时间和精力去理解与消化。</p><p>理解事物时，我们都习惯从感性认知入手，可要从感性认知进化为理性思辨，你还是不得不和那些恼人的符号和讨厌的公式打交道。然而这是学习的必经之路：直观而具体的认识虽然容易理解，其适用范围却相当有限，要解决现实问题就必须将认识上升到知识的高度，而知识的价值恰恰就蕴含在复杂的公式所体现出的规律之中。</p><p><strong>具有普适性的抽象规律，才具有学习的价值</strong>。在机器学习中，各种各样的模型某种程度而言其实也是简单具体的实例，诸如局部化和集成化之类的方法才是支配模型演变的规律。正是这些规律与统计学习的理论相结合，才让机器学习变得魅力无穷。</p><h2 id="收获-见贤思齐多自省" tabindex="-1"><a class="header-anchor" href="#收获-见贤思齐多自省"><span>收获：见贤思齐多自省</span></a></h2><p>工作上的职责所在让我接触了很多关于教学的文献与范例，其中一些国内外教学名家的课程堪称醍醐灌顶。虽然学科有所区别，但这些大师总能深入浅出、化繁为简，将深奥的道理以老妪能懂的形式清晰而准确地解释出来。体验这些大师的授课是种享受，在艰辛的求索中感受到一丝如沐春风的惬意。</p><p>罗马不是一天建成的，大师们的举重若轻也是来源于多年积累的深厚功底。博学多闻才能融会贯通，只有将广博的专业知识和精湛的教学技艺相结合，方能达到这样的境界。在我自己的角度看，从这个专栏得到的最大收获便是一份鞭策，它在不断提醒我对自己的提升依然任重而道远。</p><h2 id="启示-莫道前路多崎岖" tabindex="-1"><a class="header-anchor" href="#启示-莫道前路多崎岖"><span>启示：莫道前路多崎岖</span></a></h2><p>最近几年，所谓的“一万小时”理论声名鹊起，甚至被人奉为圭臬。可是在我看来，它无非说明了一个再简单不过的道理：有付出才有收获。究竟练习了一万小时还是两万小时并不是关键，关键在于填满这时间的努力。如果你天赋异禀外加勤于思考，两千个小时可能就足以成为高手；可要是像学弈时净想着射落天鹅的那个小孩一样，怕是十万个小时也是白搭。</p><p><strong>之所以要花费这么多的努力和时间，是因为没有任何学问是简单的</strong>。如果以玩儿票的态度对待新知，那大可不必为此大费思量；可是要深入学习一门学问的话，这样的痛苦就是必经之路，奢求速成的捷径百分之百徒劳无功。</p><p>任何一个成熟的学科都是诸多天才前辈智慧的结晶，如果这些天赋异禀之人尚且需要劈波斩浪，平凡的我辈便只有更加努力，才能在浩瀚的学海中求得生存。只有经过一波又一波惊涛骇浪的洗礼，才有资格去欣赏对岸无双的美景。</p><p>不经历风雨，怎能见彩虹，没有人能随随便便成功。终有一天，你将为今天的付出骄傲，加油！</p><p><a href="http://geektime.mikecrm.com/yweliWa" target="_blank" rel="noopener noreferrer"><img src="https://static001.geekbang.org/resource/image/f6/34/f691d4aa61d15c576d5a2128d6a95134.jpg" alt=""></a></p>',16)]))}const d=t(p,[["render",n]]),E=JSON.parse('{"path":"/posts/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A040%E8%AE%B2/%E7%BB%93%E6%9D%9F%E8%AF%AD/%E7%BB%93%E8%AF%BE%20_%20%E7%BB%88%E6%9C%89%E4%B8%80%E5%A4%A9%EF%BC%8C%E4%BD%A0%E5%B0%86%E4%B8%BA%E4%BB%8A%E5%A4%A9%E7%9A%84%E4%BB%98%E5%87%BA%E9%AA%84%E5%82%B2.html","title":"结课 _ 终有一天，你将为今天的付出骄傲","lang":"zh-CN","frontmatter":{"description":"结课 _ 终有一天，你将为今天的付出骄傲 不知不觉间，又一个40期的机器学习专栏也走到了尾声。在专栏里，我从理解概率的两大流派入手，以每种流派中的各个模型为主线，对统计机器学习和贝叶斯机器学习做了系统的介绍，并从这些模型中梳理出它们之间关系的脉络，帮助你尽可能地从更加宏观的角度来理解模型内部的关联。 内容：由博返约求精深 和上一季的“人工智能基础课”相...","head":[["meta",{"property":"og:url","content":"https://houbb.github.io/interview/posts/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A040%E8%AE%B2/%E7%BB%93%E6%9D%9F%E8%AF%AD/%E7%BB%93%E8%AF%BE%20_%20%E7%BB%88%E6%9C%89%E4%B8%80%E5%A4%A9%EF%BC%8C%E4%BD%A0%E5%B0%86%E4%B8%BA%E4%BB%8A%E5%A4%A9%E7%9A%84%E4%BB%98%E5%87%BA%E9%AA%84%E5%82%B2.html"}],["meta",{"property":"og:site_name","content":"老马啸西风"}],["meta",{"property":"og:title","content":"结课 _ 终有一天，你将为今天的付出骄傲"}],["meta",{"property":"og:description","content":"结课 _ 终有一天，你将为今天的付出骄傲 不知不觉间，又一个40期的机器学习专栏也走到了尾声。在专栏里，我从理解概率的两大流派入手，以每种流派中的各个模型为主线，对统计机器学习和贝叶斯机器学习做了系统的介绍，并从这些模型中梳理出它们之间关系的脉络，帮助你尽可能地从更加宏观的角度来理解模型内部的关联。 内容：由博返约求精深 和上一季的“人工智能基础课”相..."}],["meta",{"property":"og:type","content":"article"}],["meta",{"property":"og:locale","content":"zh-CN"}],["meta",{"property":"og:updated_time","content":"2025-08-16T11:19:38.000Z"}],["meta",{"property":"article:modified_time","content":"2025-08-16T11:19:38.000Z"}],["script",{"type":"application/ld+json"},"{\\"@context\\":\\"https://schema.org\\",\\"@type\\":\\"Article\\",\\"headline\\":\\"结课 _ 终有一天，你将为今天的付出骄傲\\",\\"image\\":[\\"\\"],\\"dateModified\\":\\"2025-08-16T11:19:38.000Z\\",\\"author\\":[{\\"@type\\":\\"Person\\",\\"name\\":\\"老马啸西风\\",\\"url\\":\\"https://houbb.github.io\\"}]}"]]},"git":{"createdTime":1755343178000,"updatedTime":1755343178000,"contributors":[{"name":"bbhou","username":"bbhou","email":"1557740299@qq.com","commits":1,"url":"https://github.com/bbhou"}]},"readingTime":{"minutes":3.78,"words":1135},"filePathRelative":"posts/机器学习40讲/结束语/结课 _ 终有一天，你将为今天的付出骄傲.md","localizedDate":"2025年8月16日","excerpt":"\\n<p><audio id=\\"audio\\" title=\\"结课 | 终有一天，你将为今天的付出骄傲\\" controls=\\"\\" preload=\\"none\\"><source id=\\"mp3\\" src=\\"https://static001.geekbang.org/resource/audio/6f/46/6f79a1684d6e01a56f79d89345d95c46.mp3\\"></audio></p>\\n<p>不知不觉间，又一个40期的机器学习专栏也走到了尾声。在专栏里，我从理解概率的两大流派入手，以每种流派中的各个模型为主线，对统计机器学习和贝叶斯机器学习做了系统的介绍，并从这些模型中梳理出它们之间关系的脉络，帮助你尽可能地从更加宏观的角度来理解模型内部的关联。</p>","autoDesc":true}');export{d as comp,E as data};
