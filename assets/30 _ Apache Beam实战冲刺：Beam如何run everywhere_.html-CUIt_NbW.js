import{_ as a}from"./plugin-vue_export-helper-DlAUqK2U.js";import{c as n,a as i,o as s}from"./app-d8EKP-K0.js";const p={};function l(r,e){return s(),n("div",null,e[0]||(e[0]=[i(`<h1 id="_30-apache-beam实战冲刺-beam如何run-everywhere" tabindex="-1"><a class="header-anchor" href="#_30-apache-beam实战冲刺-beam如何run-everywhere"><span>30 _ Apache Beam实战冲刺：Beam如何run everywhere_</span></a></h1><p><audio id="audio" title="30 | Apache Beam实战冲刺：Beam如何run everywhere?" controls="" preload="none"><source id="mp3" src="https://static001.geekbang.org/resource/audio/1f/38/1fe60ac4b76f46a5e8b3fb6f2f7e1638.mp3"></audio></p><p>你好，我是蔡元楠。</p><p>今天我要与你分享的主题是“Apache Beam实战冲刺：Beam如何run everywhere”。</p><p>你可能已经注意到，自第26讲到第29讲，从Pipeline的输入输出，到Pipeline的设计，再到Pipeline的测试，Beam Pipeline的概念一直贯穿着文章脉络。那么这一讲，我们一起来看看一个完整的Beam Pipeline究竟是如何编写的。</p><h2 id="beam-pipeline" tabindex="-1"><a class="header-anchor" href="#beam-pipeline"><span>Beam Pipeline</span></a></h2><p>一个Pipeline，或者说是一个数据处理任务，基本上都会包含以下三个步骤：</p><ol><li>读取输入数据到PCollection。</li><li>对读进来的PCollection做某些操作（也就是Transform），得到另一个PCollection。</li><li>输出你的结果PCollection。</li></ol><p>这么说，看起来很简单，但你可能会有些迷惑：这些步骤具体该怎么做呢？其实这些步骤具体到Pipeline的实际编程中，就会包含以下这些代码模块：</p><p>Java</p><div class="language- line-numbers-mode" data-highlighter="shiki" data-ext="" style="--shiki-light:#383A42;--shiki-dark:#abb2bf;--shiki-light-bg:#FAFAFA;--shiki-dark-bg:#282c34;"><pre class="shiki shiki-themes one-light one-dark-pro vp-code"><code><span class="line"><span>// Start by defining the options for the pipeline.</span></span>
<span class="line"><span>PipelineOptions options = PipelineOptionsFactory.create();</span></span>
<span class="line"><span></span></span>
<span class="line"><span>// Then create the pipeline.</span></span>
<span class="line"><span>Pipeline pipeline = Pipeline.create(options);</span></span>
<span class="line"><span></span></span>
<span class="line"><span>PCollection&amp;lt;String&amp;gt; lines = pipeline.apply(</span></span>
<span class="line"><span>  &amp;quot;ReadLines&amp;quot;, TextIO.read().from(&amp;quot;gs://some/inputData.txt&amp;quot;));</span></span>
<span class="line"><span></span></span>
<span class="line"><span>PCollection&amp;lt;String&amp;gt; filteredLines = lines.apply(new FilterLines());</span></span>
<span class="line"><span></span></span>
<span class="line"><span>filteredLines.apply(&amp;quot;WriteMyFile&amp;quot;, TextIO.write().to(&amp;quot;gs://some/outputData.txt&amp;quot;));</span></span>
<span class="line"><span></span></span>
<span class="line"><span>pipeline.run().waitUntilFinish();</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>从上面的代码例子中你可以看到，第一行和第二行代码是创建Pipeline实例。任何一个Beam程序都需要先创建一个Pipeline的实例。Pipeline实例就是用来表达Pipeline类型的对象。这里你需要注意，一个二进制程序可以动态包含多个Pipeline实例。</p><p>还是以之前的美团外卖电动车处理的例子来做说明吧。</p><p>比如，我们的程序可以动态判断是否存在第三方的电动车图片，只有当有需要处理图片时，我们才去创建一个Pipeline实例处理。我们也可以动态判断是否存在需要转换图片格式，有需要时，我们再去创建第二个Pipeline实例。这时候你的二进制程序，可能包含0个、1个，或者是2个Pipeline实例。每一个实例都是独立的，它封装了你要进行操作的数据，和你要进行的操作Transform。</p><p>Pipeline实例的创建是使用Pipeline.create(options)这个方法。其中options是传递进去的参数，options是一个PipelineOptions这个类的实例。我们会在后半部分展开PipelineOptions的丰富变化。</p><p>第三行代码，我们用TextIO.read()这个Transform读取了来自外部文本文件的内容，把所有的行表示为一个PCollection。</p><p>第四行代码，用 lines.apply(new FilterLines()) 对读进来的PCollection进行了过滤操作。</p><p>第五行代码 filteredLines.apply(“WriteMyFile”, TextIO.write().to(“gs://some/outputData.txt”))，表示把最终的PCollection结果输出到另一个文本文件。</p><p>程序运行到第五行的时候，是不是我们的数据处理任务就完成了呢？并不是。</p><p>记得我们在第24讲、第25讲中提过，Beam是延迟运行的。程序跑到第五行的时候，只是构建了Beam所需要的数据处理DAG用来优化和分配计算资源，真正的运算完全没有发生。</p><p>所以，我们需要最后一行pipeline.run().waitUntilFinish()，这才是数据真正开始被处理的语句。</p><p>这时候运行我们的代码，是不是就大功告成呢？别急，我们还没有处理好程序在哪里运行的问题。你一定会好奇，我们的程序究竟在哪里运行，不是说好了分布式数据处理吗？</p><p>在上一讲《如何测试Beam Pipeline》中我们学会了在单元测试环境中运行Beam Pipeline。就如同下面的代码。和上文的代码类似，我们把Pipeline.create(options)替换成了TestPipeline.create()。</p><p>Java</p><div class="language- line-numbers-mode" data-highlighter="shiki" data-ext="" style="--shiki-light:#383A42;--shiki-dark:#abb2bf;--shiki-light-bg:#FAFAFA;--shiki-dark-bg:#282c34;"><pre class="shiki shiki-themes one-light one-dark-pro vp-code"><code><span class="line"><span>Pipeline p = TestPipeline.create();</span></span>
<span class="line"><span></span></span>
<span class="line"><span>PCollection&amp;lt;String&amp;gt; input = p.apply(Create.of(WORDS)).setCoder(StringUtf8Coder.of());</span></span>
<span class="line"><span></span></span>
<span class="line"><span>PCollection&amp;lt;String&amp;gt; output = input.apply(new CountWords());</span></span>
<span class="line"><span></span></span>
<span class="line"><span>PAssert.that(output).containsInAnyOrder(COUNTS_ARRAY);</span></span>
<span class="line"><span></span></span>
<span class="line"><span>p.run();</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>TestPipeline是Beam Pipeline中特殊的一种，让你能够在单机上运行小规模的数据集。之前我们在分析Beam的设计理念时提到过，Beam想要把应用层的数据处理业务逻辑和底层的运算引擎分离开来。</p><p>现如今Beam可以做到让你的Pipeline代码无需修改，就可以在本地、Spark、Flink，或者在Google Cloud DataFlow上运行。这些都是通过Pipeline.create(options) 这行代码中传递的PipelineOptions实现的。</p><p>在实战中，我们应用到的所有option其实都是实现了PipelineOptions这个接口。</p><p>举个例子，如果我们希望将数据流水线放在Spark这个底层数据引擎运行的时候，我们便可以使用SparkPipelineOptions。如果我们想把数据流水线放在Flink上运行，就可以使用FlinkPipelineOptions。而这些都是extends了PipelineOptions的接口，示例如下：</p><p>Java</p><div class="language- line-numbers-mode" data-highlighter="shiki" data-ext="" style="--shiki-light:#383A42;--shiki-dark:#abb2bf;--shiki-light-bg:#FAFAFA;--shiki-dark-bg:#282c34;"><pre class="shiki shiki-themes one-light one-dark-pro vp-code"><code><span class="line"><span>options = PipelineOptionsFactory.as(SparkPipelineOptions.class);</span></span>
<span class="line"><span>Pipeline pipeline = Pipeline.create(options);</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div></div></div><p>通常一个PipelineOption是用PipelineOptionsFactory这个工厂类来创建的，它提供了两个静态工厂方法给我们去创建，<a href="http://xn--PipelineOptionsFactory-u350bnza9377a.as" target="_blank" rel="noopener noreferrer">分别是PipelineOptionsFactory.as</a>(Class)和PipelineOptionsFactory.create()。<a href="http://xn--PipelineOptionsFactory-ky68a22k8xewukb86foq0c6z4efsis3ldzj203x.as" target="_blank" rel="noopener noreferrer">像上面的示例代码就是用PipelineOptionsFactory.as</a>(Class)这个静态工厂方法来创建的。</p><p>当然了，更加常见的创建方法是从命令行中读取参数来创建PipelineOption，使用的是PipelineOptionsFactory#fromArgs(String[])这个方法，例如：</p><p>Java</p><div class="language- line-numbers-mode" data-highlighter="shiki" data-ext="" style="--shiki-light:#383A42;--shiki-dark:#abb2bf;--shiki-light-bg:#FAFAFA;--shiki-dark-bg:#282c34;"><pre class="shiki shiki-themes one-light one-dark-pro vp-code"><code><span class="line"><span>public static void main(String[] args) {</span></span>
<span class="line"><span>     PipelineOptions options = PipelineOptionsFactory.fromArgs(args).create();</span></span>
<span class="line"><span>     Pipeline p = Pipeline.create(options);</span></span>
<span class="line"><span>}</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>下面我们来看看不同的运行模式的具体使用方法。</p><h2 id="直接运行模式" tabindex="-1"><a class="header-anchor" href="#直接运行模式"><span>直接运行模式</span></a></h2><p>我们先从直接运行模式开始讲。这是我们在本地进行测试，或者调试时倾向使用的模式。在直接运行模式的时候，Beam会在单机上用多线程来模拟分布式的并行处理。</p><p>使用Java Beam SDK时，我们要给程序添加Direct Runner的依赖关系。在下面这个maven依赖关系定义文件中，我们指定了beam-runners-direct-java这样一个依赖关系。</p><div class="language- line-numbers-mode" data-highlighter="shiki" data-ext="" style="--shiki-light:#383A42;--shiki-dark:#abb2bf;--shiki-light-bg:#FAFAFA;--shiki-dark-bg:#282c34;"><pre class="shiki shiki-themes one-light one-dark-pro vp-code"><code><span class="line"><span>pom.xml</span></span>
<span class="line"><span>&amp;lt;dependency&amp;gt;</span></span>
<span class="line"><span>   &amp;lt;groupId&amp;gt;org.apache.beam&amp;lt;/groupId&amp;gt;</span></span>
<span class="line"><span>   &amp;lt;artifactId&amp;gt;beam-runners-direct-java&amp;lt;/artifactId&amp;gt;</span></span>
<span class="line"><span>   &amp;lt;version&amp;gt;2.13.0&amp;lt;/version&amp;gt;</span></span>
<span class="line"><span>   &amp;lt;scope&amp;gt;runtime&amp;lt;/scope&amp;gt;</span></span>
<span class="line"><span>&amp;lt;/dependency&amp;gt;</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>一般我们会把runner通过命令行指令传递进程序。就需要使用PipelineOptionsFactory.fromArgs(args)来创建PipelineOptions。PipelineOptionsFactory.fromArgs()是一个工厂方法，能够根据命令行参数选择生成不同的PipelineOptions子类。</p><div class="language- line-numbers-mode" data-highlighter="shiki" data-ext="" style="--shiki-light:#383A42;--shiki-dark:#abb2bf;--shiki-light-bg:#FAFAFA;--shiki-dark-bg:#282c34;"><pre class="shiki shiki-themes one-light one-dark-pro vp-code"><code><span class="line"><span>PipelineOptions options =</span></span>
<span class="line"><span>       PipelineOptionsFactory.fromArgs(args).create();</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div></div></div><p>在实验程序中也可以强行使用Direct Runner。比如：</p><div class="language- line-numbers-mode" data-highlighter="shiki" data-ext="" style="--shiki-light:#383A42;--shiki-dark:#abb2bf;--shiki-light-bg:#FAFAFA;--shiki-dark-bg:#282c34;"><pre class="shiki shiki-themes one-light one-dark-pro vp-code"><code><span class="line"><span>PipelineOptions options = PipelineOptionsFactory.create();</span></span>
<span class="line"><span>options.setRunner(DirectRunner.class);</span></span>
<span class="line"><span>// 或者这样</span></span>
<span class="line"><span>options = PipelineOptionsFactory.as(DirectRunner.class);</span></span>
<span class="line"><span>Pipeline pipeline = Pipeline.create(options);</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>如果是在命令行中指定Runner的话，那么在调用这个程序时候，需要指定这样一个参数–runner=DirectRunner。比如：</p><div class="language- line-numbers-mode" data-highlighter="shiki" data-ext="" style="--shiki-light:#383A42;--shiki-dark:#abb2bf;--shiki-light-bg:#FAFAFA;--shiki-dark-bg:#282c34;"><pre class="shiki shiki-themes one-light one-dark-pro vp-code"><code><span class="line"><span>mvn compile exec:java -Dexec.mainClass=YourMainClass \\</span></span>
<span class="line"><span>     -Dexec.args=&amp;quot;--runner=DirectRunner&amp;quot; -Pdirect-runner</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div></div></div><h2 id="spark运行模式" tabindex="-1"><a class="header-anchor" href="#spark运行模式"><span>Spark运行模式</span></a></h2><p>如果我们希望将数据流水线放在Spark这个底层数据引擎运行的时候，我们便可以使用Spark Runner。Spark Runner执行Beam程序时，能够像原生的Spark程序一样。比如，在Spark本地模式部署应用，跑在Spark的RM上，或者用YARN。</p><p>Spark Runner为在Apache Spark上运行Beam Pipeline提供了以下功能：</p><ol><li>Batch 和streaming的数据流水线；</li><li>和原生RDD和DStream一样的容错保证；</li><li>和原生Spark同样的安全性能；</li><li>可以用Spark的数据回报系统；</li><li>使用Spark Broadcast实现的Beam side-input。</li></ol><p>目前使用Spark Runner必须使用Spark 2.2版本以上。</p><p>这里，我们先添加beam-runners-spark的依赖关系。</p><div class="language- line-numbers-mode" data-highlighter="shiki" data-ext="" style="--shiki-light:#383A42;--shiki-dark:#abb2bf;--shiki-light-bg:#FAFAFA;--shiki-dark-bg:#282c34;"><pre class="shiki shiki-themes one-light one-dark-pro vp-code"><code><span class="line"><span>&amp;lt;dependency&amp;gt;</span></span>
<span class="line"><span>  &amp;lt;groupId&amp;gt;org.apache.beam&amp;lt;/groupId&amp;gt;</span></span>
<span class="line"><span>  &amp;lt;artifactId&amp;gt;beam-runners-spark&amp;lt;/artifactId&amp;gt;</span></span>
<span class="line"><span>  &amp;lt;version&amp;gt;2.13.0&amp;lt;/version&amp;gt;</span></span>
<span class="line"><span>&amp;lt;/dependency&amp;gt;</span></span>
<span class="line"><span>&amp;lt;dependency&amp;gt;</span></span>
<span class="line"><span>  &amp;lt;groupId&amp;gt;org.apache.spark&amp;lt;/groupId&amp;gt;</span></span>
<span class="line"><span>  &amp;lt;artifactId&amp;gt;spark-core_2.10&amp;lt;/artifactId&amp;gt;</span></span>
<span class="line"><span>  &amp;lt;version&amp;gt;\${spark.version}&amp;lt;/version&amp;gt;</span></span>
<span class="line"><span>&amp;lt;/dependency&amp;gt;</span></span>
<span class="line"><span>&amp;lt;dependency&amp;gt;</span></span>
<span class="line"><span>  &amp;lt;groupId&amp;gt;org.apache.spark&amp;lt;/groupId&amp;gt;</span></span>
<span class="line"><span>  &amp;lt;artifactId&amp;gt;spark-streaming_2.10&amp;lt;/artifactId&amp;gt;</span></span>
<span class="line"><span>  &amp;lt;version&amp;gt;\${spark.version}&amp;lt;/version&amp;gt;</span></span>
<span class="line"><span>&amp;lt;/dependency&amp;gt;</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>然后，要使用SparkPipelineOptions传递进Pipeline.create()方法。常见的创建方法是从命令行中读取参数来创建PipelineOption，使用的是PipelineOptionsFactory.fromArgs(String[])这个方法。在命令行中，你需要指定runner=SparkRunner：</p><div class="language- line-numbers-mode" data-highlighter="shiki" data-ext="" style="--shiki-light:#383A42;--shiki-dark:#abb2bf;--shiki-light-bg:#FAFAFA;--shiki-dark-bg:#282c34;"><pre class="shiki shiki-themes one-light one-dark-pro vp-code"><code><span class="line"><span>mvn exec:java -Dexec.mainClass=YourMainClass \\</span></span>
<span class="line"><span>    -Pspark-runner \\</span></span>
<span class="line"><span>    -Dexec.args=&amp;quot;--runner=SparkRunner \\</span></span>
<span class="line"><span>      --sparkMaster=&amp;lt;spark master url&amp;gt;&amp;quot;</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>也可以在Spark的独立集群上运行，这时候spark的提交命令，spark-submit。</p><div class="language- line-numbers-mode" data-highlighter="shiki" data-ext="" style="--shiki-light:#383A42;--shiki-dark:#abb2bf;--shiki-light-bg:#FAFAFA;--shiki-dark-bg:#282c34;"><pre class="shiki shiki-themes one-light one-dark-pro vp-code"><code><span class="line"><span>spark-submit --class YourMainClass --master spark://HOST:PORT target/...jar --runner=SparkRunner</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div></div></div><p>当Beam程序在Spark上运行时，你也可以同样用Spark的网页监控数据流水线进度。</p><h2 id="flink运行模式" tabindex="-1"><a class="header-anchor" href="#flink运行模式"><span>Flink运行模式</span></a></h2><p>Flink Runner是Beam提供的用来在Flink上运行Beam Pipeline的模式。你可以选择在计算集群上比如 Yarn/Kubernetes/Mesos 或者本地Flink上运行。Flink Runner适合大规模，连续的数据处理任务，包含了以下功能：</p><ol><li>以Streaming为中心，支持streaming处理和batch处理；</li><li>和flink一样的容错性，和exactly-once的处理语义；</li><li>可以自定义内存管理模型；</li><li>和其他（例如YARN）的Apache Hadoop生态整合比较好。</li></ol><p>其实看到这里，你可能已经掌握了这里面的诀窍。就是通过PipelineOptions来指定runner，而你的数据处理代码不需要修改。PipelineOptions可以通过命令行参数指定。那么类似Spark Runner，你也可以使用Flink来运行Beam程序。</p><p>同样的，首先你需要在pom.xml中添加Flink Runner的依赖。</p><div class="language- line-numbers-mode" data-highlighter="shiki" data-ext="" style="--shiki-light:#383A42;--shiki-dark:#abb2bf;--shiki-light-bg:#FAFAFA;--shiki-dark-bg:#282c34;"><pre class="shiki shiki-themes one-light one-dark-pro vp-code"><code><span class="line"><span>&amp;lt;dependency&amp;gt;</span></span>
<span class="line"><span>  &amp;lt;groupId&amp;gt;org.apache.beam&amp;lt;/groupId&amp;gt;</span></span>
<span class="line"><span>  &amp;lt;artifactId&amp;gt;beam-runners-flink-1.6&amp;lt;/artifactId&amp;gt;</span></span>
<span class="line"><span>  &amp;lt;version&amp;gt;2.13.0&amp;lt;/version&amp;gt;</span></span>
<span class="line"><span>&amp;lt;/dependency&amp;gt;</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>然后在命令行中指定flink runner：</p><div class="language- line-numbers-mode" data-highlighter="shiki" data-ext="" style="--shiki-light:#383A42;--shiki-dark:#abb2bf;--shiki-light-bg:#FAFAFA;--shiki-dark-bg:#282c34;"><pre class="shiki shiki-themes one-light one-dark-pro vp-code"><code><span class="line"><span>mvn exec:java -Dexec.mainClass=YourMainClass \\</span></span>
<span class="line"><span>    -Pflink-runner \\</span></span>
<span class="line"><span>    -Dexec.args=&amp;quot;--runner=FlinkRunner \\</span></span>
<span class="line"><span>      --flinkMaster=&amp;lt;flink master url&amp;gt;&amp;quot;</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><h2 id="google-dataflow-运行模式" tabindex="-1"><a class="header-anchor" href="#google-dataflow-运行模式"><span>Google Dataflow 运行模式</span></a></h2><p>Beam Pipeline也能直接在云端运行。Google Cloud Dataflow就是完全托管的Beam Runner。当你使用Google Cloud Dataflow服务来运行Beam Pipeline时，它会先上传你的二进制程序到Google Cloud，随后自动分配计算资源创建Cloud Dataflow任务。</p><p>同前面讲到的Direct Runner和Spark Runner类似，你还是需要为Cloud Dataflow添加beam-runners-google-cloud-dataflow-java依赖关系：</p><div class="language- line-numbers-mode" data-highlighter="shiki" data-ext="" style="--shiki-light:#383A42;--shiki-dark:#abb2bf;--shiki-light-bg:#FAFAFA;--shiki-dark-bg:#282c34;"><pre class="shiki shiki-themes one-light one-dark-pro vp-code"><code><span class="line"><span>&amp;lt;dependency&amp;gt;</span></span>
<span class="line"><span>  &amp;lt;groupId&amp;gt;org.apache.beam&amp;lt;/groupId&amp;gt;</span></span>
<span class="line"><span>  &amp;lt;artifactId&amp;gt;beam-runners-google-cloud-dataflow-java&amp;lt;/artifactId&amp;gt;</span></span>
<span class="line"><span>  &amp;lt;version&amp;gt;2.13.0&amp;lt;/version&amp;gt;</span></span>
<span class="line"><span>  &amp;lt;scope&amp;gt;runtime&amp;lt;/scope&amp;gt;</span></span>
<span class="line"><span>&amp;lt;/dependency&amp;gt;</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>我们假设你已经在Google Cloud上创建了project，那么就可以用类似的命令行提交任务：</p><div class="language- line-numbers-mode" data-highlighter="shiki" data-ext="" style="--shiki-light:#383A42;--shiki-dark:#abb2bf;--shiki-light-bg:#FAFAFA;--shiki-dark-bg:#282c34;"><pre class="shiki shiki-themes one-light one-dark-pro vp-code"><code><span class="line"><span>mvn -Pdataflow-runner compile exec:java \\</span></span>
<span class="line"><span>      -Dexec.mainClass=&amp;lt;YourMainClass&amp;gt; \\</span></span>
<span class="line"><span>      -Dexec.args=&amp;quot;--project=&amp;lt;PROJECT_ID&amp;gt; \\</span></span>
<span class="line"><span>      --stagingLocation=gs://&amp;lt;STORAGE_BUCKET&amp;gt;/staging/ \\</span></span>
<span class="line"><span>      --output=gs://&amp;lt;STORAGE_BUCKET&amp;gt;/output \\</span></span>
<span class="line"><span>      --runner=DataflowRunner&amp;quot;</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><h2 id="小结" tabindex="-1"><a class="header-anchor" href="#小结"><span>小结</span></a></h2><p>这一讲我们先总结了前面几讲Pipeline的完整使用方法。之后一起探索了Beam的重要特性，就是Pipeline可以通过PipelineOption动态选择同样的数据处理流水线在哪里运行。并且，分别展开讲解了直接运行模式、Spark运行模式、Flink运行模式和Google Cloud Dataflow运行模式。在实践中，你可以根据自身需要，去选择不同的运行模式。</p><h2 id="思考题" tabindex="-1"><a class="header-anchor" href="#思考题"><span>思考题</span></a></h2><p>Beam的设计模式是对计算引擎动态选择，它为什么要这么设计？</p><p>欢迎你把答案写在留言区，与我和其他同学一起讨论。如果你觉得有所收获，也欢迎把文章分享给你的朋友。</p>`,77)]))}const c=a(p,[["render",l]]),o=JSON.parse('{"path":"/posts/%E5%A4%A7%E8%A7%84%E6%A8%A1%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86%E5%AE%9E%E6%88%98/%E6%A8%A1%E5%9D%97%E4%BA%94%20_%20%E5%86%B3%E6%88%98%20Apache%20Beam%20%E7%9C%9F%E5%AE%9E%E7%A1%85%E8%B0%B7%E6%A1%88%E4%BE%8B/30%20_%20Apache%20Beam%E5%AE%9E%E6%88%98%E5%86%B2%E5%88%BA%EF%BC%9ABeam%E5%A6%82%E4%BD%95run%20everywhere_.html","title":"30 _ Apache Beam实战冲刺：Beam如何run everywhere_","lang":"zh-CN","frontmatter":{"description":"30 _ Apache Beam实战冲刺：Beam如何run everywhere_ 你好，我是蔡元楠。 今天我要与你分享的主题是“Apache Beam实战冲刺：Beam如何run everywhere”。 你可能已经注意到，自第26讲到第29讲，从Pipeline的输入输出，到Pipeline的设计，再到Pipeline的测试，Beam Pipel...","head":[["meta",{"property":"og:url","content":"https://houbb.github.io/interview/posts/%E5%A4%A7%E8%A7%84%E6%A8%A1%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86%E5%AE%9E%E6%88%98/%E6%A8%A1%E5%9D%97%E4%BA%94%20_%20%E5%86%B3%E6%88%98%20Apache%20Beam%20%E7%9C%9F%E5%AE%9E%E7%A1%85%E8%B0%B7%E6%A1%88%E4%BE%8B/30%20_%20Apache%20Beam%E5%AE%9E%E6%88%98%E5%86%B2%E5%88%BA%EF%BC%9ABeam%E5%A6%82%E4%BD%95run%20everywhere_.html"}],["meta",{"property":"og:site_name","content":"老马啸西风"}],["meta",{"property":"og:title","content":"30 _ Apache Beam实战冲刺：Beam如何run everywhere_"}],["meta",{"property":"og:description","content":"30 _ Apache Beam实战冲刺：Beam如何run everywhere_ 你好，我是蔡元楠。 今天我要与你分享的主题是“Apache Beam实战冲刺：Beam如何run everywhere”。 你可能已经注意到，自第26讲到第29讲，从Pipeline的输入输出，到Pipeline的设计，再到Pipeline的测试，Beam Pipel..."}],["meta",{"property":"og:type","content":"article"}],["meta",{"property":"og:locale","content":"zh-CN"}],["meta",{"property":"og:updated_time","content":"2025-08-16T11:19:38.000Z"}],["meta",{"property":"article:modified_time","content":"2025-08-16T11:19:38.000Z"}],["script",{"type":"application/ld+json"},"{\\"@context\\":\\"https://schema.org\\",\\"@type\\":\\"Article\\",\\"headline\\":\\"30 _ Apache Beam实战冲刺：Beam如何run everywhere_\\",\\"image\\":[\\"\\"],\\"dateModified\\":\\"2025-08-16T11:19:38.000Z\\",\\"author\\":[{\\"@type\\":\\"Person\\",\\"name\\":\\"老马啸西风\\",\\"url\\":\\"https://houbb.github.io\\"}]}"]]},"git":{"createdTime":1755343178000,"updatedTime":1755343178000,"contributors":[{"name":"bbhou","username":"bbhou","email":"1557740299@qq.com","commits":1,"url":"https://github.com/bbhou"}]},"readingTime":{"minutes":9.3,"words":2791},"filePathRelative":"posts/大规模数据处理实战/模块五 _ 决战 Apache Beam 真实硅谷案例/30 _ Apache Beam实战冲刺：Beam如何run everywhere_.md","localizedDate":"2025年8月16日","excerpt":"\\n<p><audio id=\\"audio\\" title=\\"30 | Apache Beam实战冲刺：Beam如何run everywhere?\\" controls=\\"\\" preload=\\"none\\"><source id=\\"mp3\\" src=\\"https://static001.geekbang.org/resource/audio/1f/38/1fe60ac4b76f46a5e8b3fb6f2f7e1638.mp3\\"></audio></p>\\n<p>你好，我是蔡元楠。</p>\\n<p>今天我要与你分享的主题是“Apache Beam实战冲刺：Beam如何run everywhere”。</p>","autoDesc":true}');export{c as comp,o as data};
