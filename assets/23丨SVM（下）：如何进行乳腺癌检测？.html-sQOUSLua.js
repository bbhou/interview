import{_ as a}from"./plugin-vue_export-helper-DlAUqK2U.js";import{c as e,a as n,o as i}from"./app-d8EKP-K0.js";const p={};function r(t,s){return i(),e("div",null,s[0]||(s[0]=[n(`<h1 id="_23丨svm-下-如何进行乳腺癌检测" tabindex="-1"><a class="header-anchor" href="#_23丨svm-下-如何进行乳腺癌检测"><span>23丨SVM（下）：如何进行乳腺癌检测？</span></a></h1><p><audio id="audio" title="23丨SVM（下）：如何进行乳腺癌检测？" controls="" preload="none"><source id="mp3" src="https://static001.geekbang.org/resource/audio/b3/cc/b338f169d9377c97fe6353c962591dcc.mp3"></audio></p><p>讲完了SVM的原理之后，今天我来带你进行SVM的实战。</p><p>在此之前我们先来回顾一下SVM的相关知识点。SVM是有监督的学习模型，我们需要事先对数据打上分类标签，通过求解最大分类间隔来求解二分类问题。如果要求解多分类问题，可以将多个二分类器组合起来形成一个多分类器。</p><p>上一节中讲到了硬间隔、软间隔、非线性SVM，以及分类间隔的公式，你可能会觉得比较抽象。这节课，我们会在实际使用中，讲解对工具的使用，以及相关参数的含义。</p><h2 id="如何在sklearn中使用svm" tabindex="-1"><a class="header-anchor" href="#如何在sklearn中使用svm"><span>如何在sklearn中使用SVM</span></a></h2><p>在Python的sklearn工具包中有SVM算法，首先需要引用工具包：</p><div class="language- line-numbers-mode" data-highlighter="shiki" data-ext="" style="--shiki-light:#383A42;--shiki-dark:#abb2bf;--shiki-light-bg:#FAFAFA;--shiki-dark-bg:#282c34;"><pre class="shiki shiki-themes one-light one-dark-pro vp-code"><code><span class="line"><span>from sklearn import svm</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div></div></div><p>SVM既可以做回归，也可以做分类器。</p><p>当用SVM做回归的时候，我们可以使用SVR或LinearSVR。SVR的英文是Support Vector Regression。这篇文章只讲分类，这里只是简单地提一下。</p><p>当做分类器的时候，我们使用的是SVC或者LinearSVC。SVC的英文是Support Vector Classification。</p><p>我简单说一下这两者之前的差别。</p><p>从名字上你能看出LinearSVC是个线性分类器，用于处理线性可分的数据，只能使用线性核函数。上一节，我讲到SVM是通过核函数将样本从原始空间映射到一个更高维的特质空间中，这样就使得样本在新的空间中线性可分。</p><p>如果是针对非线性的数据，需要用到SVC。在SVC中，我们既可以使用到线性核函数（进行线性划分），也能使用高维的核函数（进行非线性划分）。</p><p>如何创建一个SVM分类器呢？</p><p>我们首先使用SVC的构造函数：model = svm.SVC(kernel=‘rbf’, C=1.0, gamma=‘auto’)，这里有三个重要的参数kernel、C和gamma。</p><p>kernel代表核函数的选择，它有四种选择，只不过默认是rbf，即高斯核函数。</p><p>linear：线性核函数</p><p>poly：多项式核函数</p><p>rbf：高斯核函数（默认）</p><p>sigmoid：sigmoid核函数</p><p>这四种函数代表不同的映射方式，你可能会问，在实际工作中，如何选择这4种核函数呢？我来给你解释一下：</p><p>线性核函数，是在数据线性可分的情况下使用的，运算速度快，效果好。不足在于它不能处理线性不可分的数据。</p><p>多项式核函数可以将数据从低维空间映射到高维空间，但参数比较多，计算量大。</p><p>高斯核函数同样可以将样本映射到高维空间，但相比于多项式核函数来说所需的参数比较少，通常性能不错，所以是默认使用的核函数。</p><p>了解深度学习的同学应该知道sigmoid经常用在神经网络的映射中。因此当选用sigmoid核函数时，SVM实现的是多层神经网络。</p><p>上面介绍的4种核函数，除了第一种线性核函数外，其余3种都可以处理线性不可分的数据。</p><p>参数C代表目标函数的惩罚系数，惩罚系数指的是分错样本时的惩罚程度，默认情况下为1.0。当C越大的时候，分类器的准确性越高，但同样容错率会越低，泛化能力会变差。相反，C越小，泛化能力越强，但是准确性会降低。</p><p>参数gamma代表核函数的系数，默认为样本特征数的倒数，即gamma = 1 / n_features。</p><p>在创建SVM分类器之后，就可以输入训练集对它进行训练。我们使用model.fit(train_X,train_y)，传入训练集中的特征值矩阵train_X和分类标识train_y。特征值矩阵就是我们在特征选择后抽取的特征值矩阵（当然你也可以用全部数据作为特征值矩阵）；分类标识就是人工事先针对每个样本标识的分类结果。这样模型会自动进行分类器的训练。我们可以使用prediction=model.predict(test_X)来对结果进行预测，传入测试集中的样本特征矩阵test_X，可以得到测试集的预测分类结果prediction。</p><p>同样我们也可以创建线性SVM分类器，使用model=svm.LinearSVC()。在LinearSVC中没有kernel这个参数，限制我们只能使用线性核函数。由于LinearSVC对线性分类做了优化，对于数据量大的线性可分问题，使用LinearSVC的效率要高于SVC。</p><p>如果你不知道数据集是否为线性，可以直接使用SVC类创建SVM分类器。</p><p>在训练和预测中，LinearSVC和SVC一样，都是使用model.fit(train_X,train_y)和model.predict(test_X)。</p><h2 id="如何用svm进行乳腺癌检测" tabindex="-1"><a class="header-anchor" href="#如何用svm进行乳腺癌检测"><span>如何用SVM进行乳腺癌检测</span></a></h2><p>在了解了如何创建和使用SVM分类器后，我们来看一个实际的项目，数据集来自美国威斯康星州的乳腺癌诊断数据集，<a href="https://github.com/cystanford/breast_cancer_data/" target="_blank" rel="noopener noreferrer">点击这里进行下载</a>。</p><p>医疗人员采集了患者乳腺肿块经过细针穿刺(FNA)后的数字化图像，并且对这些数字图像进行了特征提取，这些特征可以描述图像中的细胞核呈现。肿瘤可以分成良性和恶性。部分数据截屏如下所示：</p><p><img src="https://static001.geekbang.org/resource/image/97/6a/97a33c5bfc182d571e9707db653eff6a.png" alt=""><br><br> 数据表一共包括了32个字段，代表的含义如下：</p><img src="https://static001.geekbang.org/resource/image/1e/13/1e6af6fa8bebdfba10457c111b5e9c13.jpg" alt=""><p>上面的表格中，mean代表平均值，se代表标准差，worst代表最大值（3个最大值的平均值）。每张图像都计算了相应的特征，得出了这30个特征值（不包括ID字段和分类标识结果字段diagnosis），实际上是10个特征值（radius、texture、perimeter、area、smoothness、compactness、concavity、concave points、symmetry和fractal_dimension_mean）的3个维度，平均、标准差和最大值。这些特征值都保留了4位数字。字段中没有缺失的值。在569个患者中，一共有357个是良性，212个是恶性。</p><p>好了，我们的目标是生成一个乳腺癌诊断的SVM分类器，并计算这个分类器的准确率。首先设定项目的执行流程：</p><img src="https://static001.geekbang.org/resource/image/97/f9/9768905bf3cf6d8946a64caa8575e1f9.png" alt=""><p>首先我们需要加载数据源；</p><p>在准备阶段，需要对加载的数据源进行探索，查看样本特征和特征值，这个过程你也可以使用数据可视化，它可以方便我们对数据及数据之间的关系进一步加深了解。然后按照“完全合一”的准则来评估数据的质量，如果数据质量不高就需要做数据清洗。数据清洗之后，你可以做特征选择，方便后续的模型训练；</p><p>在分类阶段，选择核函数进行训练，如果不知道数据是否为线性，可以考虑使用SVC(kernel=‘rbf’) ，也就是高斯核函数的SVM分类器。然后对训练好的模型用测试集进行评估。</p><p>按照上面的流程，我们来编写下代码，加载数据并对数据做部分的探索：</p><div class="language- line-numbers-mode" data-highlighter="shiki" data-ext="" style="--shiki-light:#383A42;--shiki-dark:#abb2bf;--shiki-light-bg:#FAFAFA;--shiki-dark-bg:#282c34;"><pre class="shiki shiki-themes one-light one-dark-pro vp-code"><code><span class="line"><span># 加载数据集，你需要把数据放到目录中</span></span>
<span class="line"><span>data = pd.read_csv(&amp;quot;./data.csv&amp;quot;)</span></span>
<span class="line"><span># 数据探索</span></span>
<span class="line"><span># 因为数据集中列比较多，我们需要把dataframe中的列全部显示出来</span></span>
<span class="line"><span>pd.set_option(&#39;display.max_columns&#39;, None)</span></span>
<span class="line"><span>print(data.columns)</span></span>
<span class="line"><span>print(data.head(5))</span></span>
<span class="line"><span>print(data.describe())</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>这是部分的运行结果，完整结果你可以自己跑一下。</p><div class="language- line-numbers-mode" data-highlighter="shiki" data-ext="" style="--shiki-light:#383A42;--shiki-dark:#abb2bf;--shiki-light-bg:#FAFAFA;--shiki-dark-bg:#282c34;"><pre class="shiki shiki-themes one-light one-dark-pro vp-code"><code><span class="line"><span>Index([&#39;id&#39;, &#39;diagnosis&#39;, &#39;radius_mean&#39;, &#39;texture_mean&#39;, &#39;perimeter_mean&#39;,</span></span>
<span class="line"><span>       &#39;area_mean&#39;, &#39;smoothness_mean&#39;, &#39;compactness_mean&#39;, &#39;concavity_mean&#39;,</span></span>
<span class="line"><span>       &#39;concave points_mean&#39;, &#39;symmetry_mean&#39;, &#39;fractal_dimension_mean&#39;,</span></span>
<span class="line"><span>       &#39;radius_se&#39;, &#39;texture_se&#39;, &#39;perimeter_se&#39;, &#39;area_se&#39;, &#39;smoothness_se&#39;,</span></span>
<span class="line"><span>       &#39;compactness_se&#39;, &#39;concavity_se&#39;, &#39;concave points_se&#39;, &#39;symmetry_se&#39;,</span></span>
<span class="line"><span>       &#39;fractal_dimension_se&#39;, &#39;radius_worst&#39;, &#39;texture_worst&#39;,</span></span>
<span class="line"><span>       &#39;perimeter_worst&#39;, &#39;area_worst&#39;, &#39;smoothness_worst&#39;,</span></span>
<span class="line"><span>       &#39;compactness_worst&#39;, &#39;concavity_worst&#39;, &#39;concave points_worst&#39;,</span></span>
<span class="line"><span>       &#39;symmetry_worst&#39;, &#39;fractal_dimension_worst&#39;],</span></span>
<span class="line"><span>      dtype=&#39;object&#39;)</span></span>
<span class="line"><span>         id diagnosis  radius_mean  texture_mean  perimeter_mean  area_mean  \\</span></span>
<span class="line"><span>0    842302         M        17.99         10.38          122.80     1001.0   </span></span>
<span class="line"><span>1    842517         M        20.57         17.77          132.90     1326.0   </span></span>
<span class="line"><span>2  84300903         M        19.69         21.25          130.00     1203.0   </span></span>
<span class="line"><span>3  84348301         M        11.42         20.38           77.58      386.1   </span></span>
<span class="line"><span>4  84358402         M        20.29         14.34          135.10     1297.0</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>接下来，我们就要对数据进行清洗了。</p><p>运行结果中，你能看到32个字段里，id是没有实际含义的，可以去掉。diagnosis字段的取值为B或者M，我们可以用0和1来替代。另外其余的30个字段，其实可以分成三组字段，下划线后面的mean、se和worst代表了每组字段不同的度量方式，分别是平均值、标准差和最大值。</p><div class="language- line-numbers-mode" data-highlighter="shiki" data-ext="" style="--shiki-light:#383A42;--shiki-dark:#abb2bf;--shiki-light-bg:#FAFAFA;--shiki-dark-bg:#282c34;"><pre class="shiki shiki-themes one-light one-dark-pro vp-code"><code><span class="line"><span># 将特征字段分成3组</span></span>
<span class="line"><span>features_mean= list(data.columns[2:12])</span></span>
<span class="line"><span>features_se= list(data.columns[12:22])</span></span>
<span class="line"><span>features_worst=list(data.columns[22:32])</span></span>
<span class="line"><span># 数据清洗</span></span>
<span class="line"><span># ID列没有用，删除该列</span></span>
<span class="line"><span>data.drop(&amp;quot;id&amp;quot;,axis=1,inplace=True)</span></span>
<span class="line"><span># 将B良性替换为0，M恶性替换为1</span></span>
<span class="line"><span>data[&#39;diagnosis&#39;]=data[&#39;diagnosis&#39;].map({&#39;M&#39;:1,&#39;B&#39;:0})</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>然后我们要做特征字段的筛选，首先需要观察下features_mean各变量之间的关系，这里我们可以用DataFrame的corr()函数，然后用热力图帮我们可视化呈现。同样，我们也会看整体良性、恶性肿瘤的诊断情况。</p><div class="language- line-numbers-mode" data-highlighter="shiki" data-ext="" style="--shiki-light:#383A42;--shiki-dark:#abb2bf;--shiki-light-bg:#FAFAFA;--shiki-dark-bg:#282c34;"><pre class="shiki shiki-themes one-light one-dark-pro vp-code"><code><span class="line"><span># 将肿瘤诊断结果可视化</span></span>
<span class="line"><span>sns.countplot(data[&#39;diagnosis&#39;],label=&amp;quot;Count&amp;quot;)</span></span>
<span class="line"><span>plt.show()</span></span>
<span class="line"><span># 用热力图呈现features_mean字段之间的相关性</span></span>
<span class="line"><span>corr = data[features_mean].corr()</span></span>
<span class="line"><span>plt.figure(figsize=(14,14))</span></span>
<span class="line"><span># annot=True显示每个方格的数据</span></span>
<span class="line"><span>sns.heatmap(corr, annot=True)</span></span>
<span class="line"><span>plt.show()</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>这是运行的结果：</p><img src="https://static001.geekbang.org/resource/image/a6/4d/a65435de48cee8091bd5f83d286ddb4d.png" alt=""><p><img src="https://static001.geekbang.org/resource/image/07/6e/0780e76fd3807759ab4881c2c39cb76e.png" alt=""><br><br> 热力图中对角线上的为单变量自身的相关系数是1。颜色越浅代表相关性越大。所以你能看出来radius_mean、perimeter_mean和area_mean相关性非常大，compactness_mean、concavity_mean、concave_points_mean这三个字段也是相关的，因此我们可以取其中的一个作为代表。</p><p>那么如何进行特征选择呢？</p><p>特征选择的目的是降维，用少量的特征代表数据的特性，这样也可以增强分类器的泛化能力，避免数据过拟合。</p><p>我们能看到mean、se和worst这三组特征是对同一组内容的不同度量方式，我们可以保留mean这组特征，在特征选择中忽略掉se和worst。同时我们能看到mean这组特征中，radius_mean、perimeter_mean、area_mean这三个属性相关性大，compactness_mean、daconcavity_mean、concave points_mean这三个属性相关性大。我们分别从这2类中选择1个属性作为代表，比如radius_mean和compactness_mean。</p><p>这样我们就可以把原来的10个属性缩减为6个属性，代码如下：</p><div class="language- line-numbers-mode" data-highlighter="shiki" data-ext="" style="--shiki-light:#383A42;--shiki-dark:#abb2bf;--shiki-light-bg:#FAFAFA;--shiki-dark-bg:#282c34;"><pre class="shiki shiki-themes one-light one-dark-pro vp-code"><code><span class="line"><span># 特征选择</span></span>
<span class="line"><span>features_remain = [&#39;radius_mean&#39;,&#39;texture_mean&#39;, &#39;smoothness_mean&#39;,&#39;compactness_mean&#39;,&#39;symmetry_mean&#39;, &#39;fractal_dimension_mean&#39;]</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div></div></div><p>对特征进行选择之后，我们就可以准备训练集和测试集：</p><div class="language- line-numbers-mode" data-highlighter="shiki" data-ext="" style="--shiki-light:#383A42;--shiki-dark:#abb2bf;--shiki-light-bg:#FAFAFA;--shiki-dark-bg:#282c34;"><pre class="shiki shiki-themes one-light one-dark-pro vp-code"><code><span class="line"><span># 抽取30%的数据作为测试集，其余作为训练集</span></span>
<span class="line"><span>train, test = train_test_split(data, test_size = 0.3)# in this our main data is splitted into train and test</span></span>
<span class="line"><span># 抽取特征选择的数值作为训练和测试数据</span></span>
<span class="line"><span>train_X = train[features_remain]</span></span>
<span class="line"><span>train_y=train[&#39;diagnosis&#39;]</span></span>
<span class="line"><span>test_X= test[features_remain]</span></span>
<span class="line"><span>test_y =test[&#39;diagnosis&#39;]</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>在训练之前，我们需要对数据进行规范化，这样让数据同在同一个量级上，避免因为维度问题造成数据误差：</p><div class="language- line-numbers-mode" data-highlighter="shiki" data-ext="" style="--shiki-light:#383A42;--shiki-dark:#abb2bf;--shiki-light-bg:#FAFAFA;--shiki-dark-bg:#282c34;"><pre class="shiki shiki-themes one-light one-dark-pro vp-code"><code><span class="line"><span># 采用Z-Score规范化数据，保证每个特征维度的数据均值为0，方差为1</span></span>
<span class="line"><span>ss = StandardScaler()</span></span>
<span class="line"><span>train_X = ss.fit_transform(train_X)</span></span>
<span class="line"><span>test_X = ss.transform(test_X)</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>最后我们可以让SVM做训练和预测了：</p><div class="language- line-numbers-mode" data-highlighter="shiki" data-ext="" style="--shiki-light:#383A42;--shiki-dark:#abb2bf;--shiki-light-bg:#FAFAFA;--shiki-dark-bg:#282c34;"><pre class="shiki shiki-themes one-light one-dark-pro vp-code"><code><span class="line"><span># 创建SVM分类器</span></span>
<span class="line"><span>model = svm.SVC()</span></span>
<span class="line"><span># 用训练集做训练</span></span>
<span class="line"><span>model.fit(train_X,train_y)</span></span>
<span class="line"><span># 用测试集做预测</span></span>
<span class="line"><span>prediction=model.predict(test_X)</span></span>
<span class="line"><span>print(&#39;准确率: &#39;, metrics.accuracy_score(test_y,prediction))</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>运行结果：</p><div class="language- line-numbers-mode" data-highlighter="shiki" data-ext="" style="--shiki-light:#383A42;--shiki-dark:#abb2bf;--shiki-light-bg:#FAFAFA;--shiki-dark-bg:#282c34;"><pre class="shiki shiki-themes one-light one-dark-pro vp-code"><code><span class="line"><span>准确率:  0.9181286549707602</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div></div></div><p>准确率大于90%，说明训练结果还不错。完整的代码你可以从<a href="https://github.com/cystanford/breast_cancer_data" target="_blank" rel="noopener noreferrer">GitHub</a>上下载。</p><h2 id="总结" tabindex="-1"><a class="header-anchor" href="#总结"><span>总结</span></a></h2><p>今天我带你一起做了乳腺癌诊断分类的SVM实战，从这个过程中你应该能体会出来整个执行的流程，包括数据加载、数据探索、数据清洗、特征选择、SVM训练和结果评估等环节。</p><p>sklearn已经为我们提供了很好的工具，对上节课中讲到的SVM的创建和训练都进行了封装，让我们无需关心中间的运算细节。但正因为这样，我们更需要对每个流程熟练掌握，通过实战项目训练数据化思维和对数据的敏感度。</p><p><img src="https://static001.geekbang.org/resource/image/79/82/797fe646ae4668139600fca2c50c5282.png" alt=""><br><br> 最后给你留两道思考题吧。还是这个乳腺癌诊断的数据，请你用LinearSVC，选取全部的特征（除了ID以外）作为训练数据，看下你的分类器能得到多少的准确度呢？另外你对sklearn中SVM使用又有什么样的体会呢？</p><p>欢迎在评论区与我分享你的答案，也欢迎点击“请朋友读”，把这篇文章分享给你的朋友或者同事，一起来交流，一起来进步。</p>`,75)]))}const c=a(p,[["render",r]]),o=JSON.parse('{"path":"/posts/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E5%AE%9E%E6%88%9845%E8%AE%B2/%E7%AC%AC%E4%BA%8C%E6%A8%A1%E5%9D%97%EF%BC%9A%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E7%AE%97%E6%B3%95%E7%AF%87/23%E4%B8%A8SVM%EF%BC%88%E4%B8%8B%EF%BC%89%EF%BC%9A%E5%A6%82%E4%BD%95%E8%BF%9B%E8%A1%8C%E4%B9%B3%E8%85%BA%E7%99%8C%E6%A3%80%E6%B5%8B%EF%BC%9F.html","title":"23丨SVM（下）：如何进行乳腺癌检测？","lang":"zh-CN","frontmatter":{"description":"23丨SVM（下）：如何进行乳腺癌检测？ 讲完了SVM的原理之后，今天我来带你进行SVM的实战。 在此之前我们先来回顾一下SVM的相关知识点。SVM是有监督的学习模型，我们需要事先对数据打上分类标签，通过求解最大分类间隔来求解二分类问题。如果要求解多分类问题，可以将多个二分类器组合起来形成一个多分类器。 上一节中讲到了硬间隔、软间隔、非线性SVM，以及...","head":[["meta",{"property":"og:url","content":"https://houbb.github.io/interview/posts/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E5%AE%9E%E6%88%9845%E8%AE%B2/%E7%AC%AC%E4%BA%8C%E6%A8%A1%E5%9D%97%EF%BC%9A%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E7%AE%97%E6%B3%95%E7%AF%87/23%E4%B8%A8SVM%EF%BC%88%E4%B8%8B%EF%BC%89%EF%BC%9A%E5%A6%82%E4%BD%95%E8%BF%9B%E8%A1%8C%E4%B9%B3%E8%85%BA%E7%99%8C%E6%A3%80%E6%B5%8B%EF%BC%9F.html"}],["meta",{"property":"og:site_name","content":"老马啸西风"}],["meta",{"property":"og:title","content":"23丨SVM（下）：如何进行乳腺癌检测？"}],["meta",{"property":"og:description","content":"23丨SVM（下）：如何进行乳腺癌检测？ 讲完了SVM的原理之后，今天我来带你进行SVM的实战。 在此之前我们先来回顾一下SVM的相关知识点。SVM是有监督的学习模型，我们需要事先对数据打上分类标签，通过求解最大分类间隔来求解二分类问题。如果要求解多分类问题，可以将多个二分类器组合起来形成一个多分类器。 上一节中讲到了硬间隔、软间隔、非线性SVM，以及..."}],["meta",{"property":"og:type","content":"article"}],["meta",{"property":"og:locale","content":"zh-CN"}],["meta",{"property":"og:updated_time","content":"2025-08-16T11:19:38.000Z"}],["meta",{"property":"article:modified_time","content":"2025-08-16T11:19:38.000Z"}],["script",{"type":"application/ld+json"},"{\\"@context\\":\\"https://schema.org\\",\\"@type\\":\\"Article\\",\\"headline\\":\\"23丨SVM（下）：如何进行乳腺癌检测？\\",\\"image\\":[\\"\\"],\\"dateModified\\":\\"2025-08-16T11:19:38.000Z\\",\\"author\\":[{\\"@type\\":\\"Person\\",\\"name\\":\\"老马啸西风\\",\\"url\\":\\"https://houbb.github.io\\"}]}"]]},"git":{"createdTime":1755343178000,"updatedTime":1755343178000,"contributors":[{"name":"bbhou","username":"bbhou","email":"1557740299@qq.com","commits":1,"url":"https://github.com/bbhou"}]},"readingTime":{"minutes":10.72,"words":3217},"filePathRelative":"posts/数据分析实战45讲/第二模块：数据分析算法篇/23丨SVM（下）：如何进行乳腺癌检测？.md","localizedDate":"2025年8月16日","excerpt":"\\n<p><audio id=\\"audio\\" title=\\"23丨SVM（下）：如何进行乳腺癌检测？\\" controls=\\"\\" preload=\\"none\\"><source id=\\"mp3\\" src=\\"https://static001.geekbang.org/resource/audio/b3/cc/b338f169d9377c97fe6353c962591dcc.mp3\\"></audio></p>\\n<p>讲完了SVM的原理之后，今天我来带你进行SVM的实战。</p>\\n<p>在此之前我们先来回顾一下SVM的相关知识点。SVM是有监督的学习模型，我们需要事先对数据打上分类标签，通过求解最大分类间隔来求解二分类问题。如果要求解多分类问题，可以将多个二分类器组合起来形成一个多分类器。</p>","autoDesc":true}');export{c as comp,o as data};
