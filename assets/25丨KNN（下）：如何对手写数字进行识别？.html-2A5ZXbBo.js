import{_ as n}from"./plugin-vue_export-helper-DlAUqK2U.js";import{c as a,a as i,o as e}from"./app-d8EKP-K0.js";const p={};function l(r,s){return e(),a("div",null,s[0]||(s[0]=[i(`<h1 id="_25丨knn-下-如何对手写数字进行识别" tabindex="-1"><a class="header-anchor" href="#_25丨knn-下-如何对手写数字进行识别"><span>25丨KNN（下）：如何对手写数字进行识别？</span></a></h1><p><audio id="audio" title="25丨KNN（下）：如何对手写数字进行识别？" controls="" preload="none"><source id="mp3" src="https://static001.geekbang.org/resource/audio/bd/11/bd87b5d9179bfd3740fd19f6b7641b11.mp3"></audio></p><p>今天我来带你进行KNN的实战。上节课，我讲了KNN实际上是计算待分类物体与其他物体之间的距离，然后通过统计最近的K个邻居的分类情况，来决定这个物体的分类情况。</p><p>这节课，我们先看下如何在sklearn中使用KNN算法，然后通过sklearn中自带的手写数字数据集来进行实战。</p><p>之前我还讲过SVM、朴素贝叶斯和决策树分类，我们还可以用这个数据集来做下训练，对比下这四个分类器的训练结果。</p><h2 id="如何在sklearn中使用knn" tabindex="-1"><a class="header-anchor" href="#如何在sklearn中使用knn"><span>如何在sklearn中使用KNN</span></a></h2><p>在Python的sklearn工具包中有KNN算法。KNN既可以做分类器，也可以做回归。如果是做分类，你需要引用：</p><div class="language- line-numbers-mode" data-highlighter="shiki" data-ext="" style="--shiki-light:#383A42;--shiki-dark:#abb2bf;--shiki-light-bg:#FAFAFA;--shiki-dark-bg:#282c34;"><pre class="shiki shiki-themes one-light one-dark-pro vp-code"><code><span class="line"><span>from sklearn.neighbors import KNeighborsClassifier</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div></div></div><p>如果是做回归，你需要引用：</p><div class="language- line-numbers-mode" data-highlighter="shiki" data-ext="" style="--shiki-light:#383A42;--shiki-dark:#abb2bf;--shiki-light-bg:#FAFAFA;--shiki-dark-bg:#282c34;"><pre class="shiki shiki-themes one-light one-dark-pro vp-code"><code><span class="line"><span>from sklearn.neighbors import KNeighborsRegressor</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div></div></div><p>从名字上你也能看出来Classifier对应的是分类，Regressor对应的是回归。一般来说如果一个算法有Classifier类，都能找到相应的Regressor类。比如在决策树分类中，你可以使用DecisionTreeClassifier，也可以使用决策树来做回归DecisionTreeRegressor。</p><p>好了，我们看下如何在sklearn中创建KNN分类器。</p><p>这里，我们使用构造函数KNeighborsClassifier(n_neighbors=5, weights=‘uniform’, algorithm=‘auto’, leaf_size=30)，这里有几个比较主要的参数，我分别来讲解下：</p><p>1.n_neighbors：即KNN中的K值，代表的是邻居的数量。K值如果比较小，会造成过拟合。如果K值比较大，无法将未知物体分类出来。一般我们使用默认值5。</p><p>2.weights：是用来确定邻居的权重，有三种方式：</p><p>weights=uniform，代表所有邻居的权重相同；</p><p>weights=distance，代表权重是距离的倒数，即与距离成反比；</p><p>自定义函数，你可以自定义不同距离所对应的权重。大部分情况下不需要自己定义函数。</p><p>3.algorithm：用来规定计算邻居的方法，它有四种方式：</p><p>algorithm=auto，根据数据的情况自动选择适合的算法，默认情况选择auto；</p><p>algorithm=kd_tree，也叫作KD树，是多维空间的数据结构，方便对关键数据进行检索，不过KD树适用于维度少的情况，一般维数不超过20，如果维数大于20之后，效率反而会下降；</p><p>algorithm=ball_tree，也叫作球树，它和KD树一样都是多维空间的数据结果，不同于KD树，球树更适用于维度大的情况；</p><p>algorithm=brute，也叫作暴力搜索，它和KD树不同的地方是在于采用的是线性扫描，而不是通过构造树结构进行快速检索。当训练集大的时候，效率很低。</p><p>4.leaf_size：代表构造KD树或球树时的叶子数，默认是30，调整leaf_size会影响到树的构造和搜索速度。</p><p>创建完KNN分类器之后，我们就可以输入训练集对它进行训练，这里我们使用fit()函数，传入训练集中的样本特征矩阵和分类标识，会自动得到训练好的KNN分类器。然后可以使用predict()函数来对结果进行预测，这里传入测试集的特征矩阵，可以得到测试集的预测分类结果。</p><h2 id="如何用knn对手写数字进行识别分类" tabindex="-1"><a class="header-anchor" href="#如何用knn对手写数字进行识别分类"><span>如何用KNN对手写数字进行识别分类</span></a></h2><p>手写数字数据集是个非常有名的用于图像识别的数据集。数字识别的过程就是将这些图片与分类结果0-9一一对应起来。完整的手写数字数据集MNIST里面包括了60000个训练样本，以及10000个测试样本。如果你学习深度学习的话，MNIST基本上是你接触的第一个数据集。</p><p>今天我们用sklearn自带的手写数字数据集做KNN分类，你可以把这个数据集理解成一个简版的MNIST数据集，它只包括了1797幅数字图像，每幅图像大小是8*8像素。</p><p>好了，我们先来规划下整个KNN分类的流程：</p><p><img src="https://static001.geekbang.org/resource/image/8a/78/8af94562f6bd3ac42036ec47f5ad2578.jpg" alt=""><br><br> 整个训练过程基本上都会包括三个阶段：</p><p>数据加载：我们可以直接从sklearn中加载自带的手写数字数据集；</p><p>准备阶段：在这个阶段中，我们需要对数据集有个初步的了解，比如样本的个数、图像长什么样、识别结果是怎样的。你可以通过可视化的方式来查看图像的呈现。通过数据规范化可以让数据都在同一个数量级的维度。另外，因为训练集是图像，每幅图像是个8*8的矩阵，我们不需要对它进行特征选择，将全部的图像数据作为特征值矩阵即可；</p><p>分类阶段：通过训练可以得到分类器，然后用测试集进行准确率的计算。</p><p>好了，按照上面的步骤，我们一起来实现下这个项目。</p><p>首先是加载数据和对数据的探索：</p><div class="language- line-numbers-mode" data-highlighter="shiki" data-ext="" style="--shiki-light:#383A42;--shiki-dark:#abb2bf;--shiki-light-bg:#FAFAFA;--shiki-dark-bg:#282c34;"><pre class="shiki shiki-themes one-light one-dark-pro vp-code"><code><span class="line"><span># 加载数据</span></span>
<span class="line"><span>digits = load_digits()</span></span>
<span class="line"><span>data = digits.data</span></span>
<span class="line"><span># 数据探索</span></span>
<span class="line"><span>print(data.shape)</span></span>
<span class="line"><span># 查看第一幅图像</span></span>
<span class="line"><span>print(digits.images[0])</span></span>
<span class="line"><span># 第一幅图像代表的数字含义</span></span>
<span class="line"><span>print(digits.target[0])</span></span>
<span class="line"><span># 将第一幅图像显示出来</span></span>
<span class="line"><span>plt.gray()</span></span>
<span class="line"><span>plt.imshow(digits.images[0])</span></span>
<span class="line"><span>plt.show()</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>运行结果：</p><div class="language- line-numbers-mode" data-highlighter="shiki" data-ext="" style="--shiki-light:#383A42;--shiki-dark:#abb2bf;--shiki-light-bg:#FAFAFA;--shiki-dark-bg:#282c34;"><pre class="shiki shiki-themes one-light one-dark-pro vp-code"><code><span class="line"><span>(1797, 64)</span></span>
<span class="line"><span>[[ 0.  0.  5. 13.  9.  1.  0.  0.]</span></span>
<span class="line"><span> [ 0.  0. 13. 15. 10. 15.  5.  0.]</span></span>
<span class="line"><span> [ 0.  3. 15.  2.  0. 11.  8.  0.]</span></span>
<span class="line"><span> [ 0.  4. 12.  0.  0.  8.  8.  0.]</span></span>
<span class="line"><span> [ 0.  5.  8.  0.  0.  9.  8.  0.]</span></span>
<span class="line"><span> [ 0.  4. 11.  0.  1. 12.  7.  0.]</span></span>
<span class="line"><span> [ 0.  2. 14.  5. 10. 12.  0.  0.]</span></span>
<span class="line"><span> [ 0.  0.  6. 13. 10.  0.  0.  0.]]</span></span>
<span class="line"><span>0</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p><img src="https://static001.geekbang.org/resource/image/62/3c/625b7e95a22c025efa545d7144ec5f3c.png" alt=""><br><br> 我们对原始数据集中的第一幅进行数据可视化，可以看到图像是个8*8的像素矩阵，上面这幅图像是一个“0”，从训练集的分类标注中我们也可以看到分类标注为“0”。</p><p>sklearn自带的手写数字数据集一共包括了1797个样本，每幅图像都是8*8像素的矩阵。因为并没有专门的测试集，所以我们需要对数据集做划分，划分成训练集和测试集。因为KNN算法和距离定义相关，我们需要对数据进行规范化处理，采用Z-Score规范化，代码如下：</p><div class="language- line-numbers-mode" data-highlighter="shiki" data-ext="" style="--shiki-light:#383A42;--shiki-dark:#abb2bf;--shiki-light-bg:#FAFAFA;--shiki-dark-bg:#282c34;"><pre class="shiki shiki-themes one-light one-dark-pro vp-code"><code><span class="line"><span># 分割数据，将25%的数据作为测试集，其余作为训练集（你也可以指定其他比例的数据作为训练集）</span></span>
<span class="line"><span>train_x, test_x, train_y, test_y = train_test_split(data, digits.target, test_size=0.25, random_state=33)</span></span>
<span class="line"><span># 采用Z-Score规范化</span></span>
<span class="line"><span>ss = preprocessing.StandardScaler()</span></span>
<span class="line"><span>train_ss_x = ss.fit_transform(train_x)</span></span>
<span class="line"><span>test_ss_x = ss.transform(test_x)</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>然后我们构造一个KNN分类器knn，把训练集的数据传入构造好的knn，并通过测试集进行结果预测，与测试集的结果进行对比，得到KNN分类器准确率，代码如下：</p><div class="language- line-numbers-mode" data-highlighter="shiki" data-ext="" style="--shiki-light:#383A42;--shiki-dark:#abb2bf;--shiki-light-bg:#FAFAFA;--shiki-dark-bg:#282c34;"><pre class="shiki shiki-themes one-light one-dark-pro vp-code"><code><span class="line"><span># 创建KNN分类器</span></span>
<span class="line"><span>knn = KNeighborsClassifier() </span></span>
<span class="line"><span>knn.fit(train_ss_x, train_y) </span></span>
<span class="line"><span>predict_y = knn.predict(test_ss_x) </span></span>
<span class="line"><span>print(&amp;quot;KNN准确率: %.4lf&amp;quot; % accuracy_score(test_y, predict_y))</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>运行结果：</p><div class="language- line-numbers-mode" data-highlighter="shiki" data-ext="" style="--shiki-light:#383A42;--shiki-dark:#abb2bf;--shiki-light-bg:#FAFAFA;--shiki-dark-bg:#282c34;"><pre class="shiki shiki-themes one-light one-dark-pro vp-code"><code><span class="line"><span>KNN准确率: 0.9756</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div></div></div><p>好了，这样我们就构造好了一个KNN分类器。之前我们还讲过SVM、朴素贝叶斯和决策树分类。我们用手写数字数据集一起来训练下这些分类器，然后对比下哪个分类器的效果更好。代码如下：</p><div class="language- line-numbers-mode" data-highlighter="shiki" data-ext="" style="--shiki-light:#383A42;--shiki-dark:#abb2bf;--shiki-light-bg:#FAFAFA;--shiki-dark-bg:#282c34;"><pre class="shiki shiki-themes one-light one-dark-pro vp-code"><code><span class="line"><span># 创建SVM分类器</span></span>
<span class="line"><span>svm = SVC()</span></span>
<span class="line"><span>svm.fit(train_ss_x, train_y)</span></span>
<span class="line"><span>predict_y=svm.predict(test_ss_x)</span></span>
<span class="line"><span>print(&#39;SVM准确率: %0.4lf&#39; % accuracy_score(test_y, predict_y))</span></span>
<span class="line"><span># 采用Min-Max规范化</span></span>
<span class="line"><span>mm = preprocessing.MinMaxScaler()</span></span>
<span class="line"><span>train_mm_x = mm.fit_transform(train_x)</span></span>
<span class="line"><span>test_mm_x = mm.transform(test_x)</span></span>
<span class="line"><span># 创建Naive Bayes分类器</span></span>
<span class="line"><span>mnb = MultinomialNB()</span></span>
<span class="line"><span>mnb.fit(train_mm_x, train_y) </span></span>
<span class="line"><span>predict_y = mnb.predict(test_mm_x) </span></span>
<span class="line"><span>print(&amp;quot;多项式朴素贝叶斯准确率: %.4lf&amp;quot; % accuracy_score(test_y, predict_y))</span></span>
<span class="line"><span># 创建CART决策树分类器</span></span>
<span class="line"><span>dtc = DecisionTreeClassifier()</span></span>
<span class="line"><span>dtc.fit(train_mm_x, train_y) </span></span>
<span class="line"><span>predict_y = dtc.predict(test_mm_x) </span></span>
<span class="line"><span>print(&amp;quot;CART决策树准确率: %.4lf&amp;quot; % accuracy_score(test_y, predict_y))</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>运行结果如下：</p><div class="language- line-numbers-mode" data-highlighter="shiki" data-ext="" style="--shiki-light:#383A42;--shiki-dark:#abb2bf;--shiki-light-bg:#FAFAFA;--shiki-dark-bg:#282c34;"><pre class="shiki shiki-themes one-light one-dark-pro vp-code"><code><span class="line"><span>SVM准确率: 0.9867</span></span>
<span class="line"><span>多项式朴素贝叶斯准确率: 0.8844</span></span>
<span class="line"><span>CART决策树准确率: 0.8556</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>这里需要注意的是，我们在做多项式朴素贝叶斯分类的时候，传入的数据不能有负数。因为Z-Score会将数值规范化为一个标准的正态分布，即均值为0，方差为1，数值会包含负数。因此我们需要采用Min-Max规范化，将数据规范化到[0,1]范围内。</p><p>好了，我们整理下这4个分类器的结果。</p><p><img src="https://static001.geekbang.org/resource/image/0f/e8/0f498e0197935bfe15d9b1209bad8fe8.png" alt=""><br><br> 你能看出来KNN的准确率还是不错的，和SVM不相上下。</p><p>你可以自己跑一遍整个代码，在运行前还需要import相关的工具包（下面的这些工具包你都会用到，所以都需要引用）：</p><div class="language- line-numbers-mode" data-highlighter="shiki" data-ext="" style="--shiki-light:#383A42;--shiki-dark:#abb2bf;--shiki-light-bg:#FAFAFA;--shiki-dark-bg:#282c34;"><pre class="shiki shiki-themes one-light one-dark-pro vp-code"><code><span class="line"><span>from sklearn.model_selection import train_test_split</span></span>
<span class="line"><span>from sklearn import preprocessing</span></span>
<span class="line"><span>from sklearn.metrics import accuracy_score</span></span>
<span class="line"><span>from sklearn.datasets import load_digits</span></span>
<span class="line"><span>from sklearn.neighbors import KNeighborsClassifier</span></span>
<span class="line"><span>from sklearn.svm import SVC</span></span>
<span class="line"><span>from sklearn.naive_bayes import MultinomialNB</span></span>
<span class="line"><span>from sklearn.tree import DecisionTreeClassifier</span></span>
<span class="line"><span>import matplotlib.pyplot as plt</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>代码中，我使用了train_test_split做数据集的拆分，使用matplotlib.pyplot工具包显示图像，使用accuracy_score进行分类器准确率的计算，使用preprocessing中的StandardScaler和MinMaxScaler做数据的规范化。</p><p>完整的代码你可以从<a href="https://github.com/cystanford/knn" target="_blank" rel="noopener noreferrer">GitHub</a>上下载。</p><h2 id="总结" tabindex="-1"><a class="header-anchor" href="#总结"><span>总结</span></a></h2><p>今天我带你一起做了手写数字分类识别的实战，分别用KNN、SVM、朴素贝叶斯和决策树做分类器，并统计了四个分类器的准确率。在这个过程中你应该对数据探索、数据可视化、数据规范化、模型训练和结果评估的使用过程有了一定的体会。在数据量不大的情况下，使用sklearn还是方便的。</p><p>如果数据量很大，比如MNIST数据集中的6万个训练数据和1万个测试数据，那么采用深度学习+GPU运算的方式会更适合。因为深度学习的特点就是需要大量并行的重复计算，GPU最擅长的就是做大量的并行计算。</p><p><img src="https://static001.geekbang.org/resource/image/d0/e1/d08f489c3bffaacb6910f32a0fa600e1.png" alt=""><br><br> 最后留两道思考题吧，请你说说项目中KNN分类器的常用构造参数，功能函数都有哪些，以及你对KNN使用的理解？如果把KNN中的K值设置为200，数据集还是sklearn中的手写数字数据集，再跑一遍程序，看看分类器的准确率是多少？</p><p>欢迎在评论区与我分享你的答案，也欢迎点击“请朋友读”，把这篇文章分享给你的朋友或者同事。</p>`,61)]))}const c=n(p,[["render",l]]),o=JSON.parse('{"path":"/posts/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E5%AE%9E%E6%88%9845%E8%AE%B2/%E7%AC%AC%E4%BA%8C%E6%A8%A1%E5%9D%97%EF%BC%9A%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E7%AE%97%E6%B3%95%E7%AF%87/25%E4%B8%A8KNN%EF%BC%88%E4%B8%8B%EF%BC%89%EF%BC%9A%E5%A6%82%E4%BD%95%E5%AF%B9%E6%89%8B%E5%86%99%E6%95%B0%E5%AD%97%E8%BF%9B%E8%A1%8C%E8%AF%86%E5%88%AB%EF%BC%9F.html","title":"25丨KNN（下）：如何对手写数字进行识别？","lang":"zh-CN","frontmatter":{"description":"25丨KNN（下）：如何对手写数字进行识别？ 今天我来带你进行KNN的实战。上节课，我讲了KNN实际上是计算待分类物体与其他物体之间的距离，然后通过统计最近的K个邻居的分类情况，来决定这个物体的分类情况。 这节课，我们先看下如何在sklearn中使用KNN算法，然后通过sklearn中自带的手写数字数据集来进行实战。 之前我还讲过SVM、朴素贝叶斯和决...","head":[["meta",{"property":"og:url","content":"https://houbb.github.io/interview/posts/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E5%AE%9E%E6%88%9845%E8%AE%B2/%E7%AC%AC%E4%BA%8C%E6%A8%A1%E5%9D%97%EF%BC%9A%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E7%AE%97%E6%B3%95%E7%AF%87/25%E4%B8%A8KNN%EF%BC%88%E4%B8%8B%EF%BC%89%EF%BC%9A%E5%A6%82%E4%BD%95%E5%AF%B9%E6%89%8B%E5%86%99%E6%95%B0%E5%AD%97%E8%BF%9B%E8%A1%8C%E8%AF%86%E5%88%AB%EF%BC%9F.html"}],["meta",{"property":"og:site_name","content":"老马啸西风"}],["meta",{"property":"og:title","content":"25丨KNN（下）：如何对手写数字进行识别？"}],["meta",{"property":"og:description","content":"25丨KNN（下）：如何对手写数字进行识别？ 今天我来带你进行KNN的实战。上节课，我讲了KNN实际上是计算待分类物体与其他物体之间的距离，然后通过统计最近的K个邻居的分类情况，来决定这个物体的分类情况。 这节课，我们先看下如何在sklearn中使用KNN算法，然后通过sklearn中自带的手写数字数据集来进行实战。 之前我还讲过SVM、朴素贝叶斯和决..."}],["meta",{"property":"og:type","content":"article"}],["meta",{"property":"og:locale","content":"zh-CN"}],["meta",{"property":"og:updated_time","content":"2025-08-16T11:19:38.000Z"}],["meta",{"property":"article:modified_time","content":"2025-08-16T11:19:38.000Z"}],["script",{"type":"application/ld+json"},"{\\"@context\\":\\"https://schema.org\\",\\"@type\\":\\"Article\\",\\"headline\\":\\"25丨KNN（下）：如何对手写数字进行识别？\\",\\"image\\":[\\"\\"],\\"dateModified\\":\\"2025-08-16T11:19:38.000Z\\",\\"author\\":[{\\"@type\\":\\"Person\\",\\"name\\":\\"老马啸西风\\",\\"url\\":\\"https://houbb.github.io\\"}]}"]]},"git":{"createdTime":1755343178000,"updatedTime":1755343178000,"contributors":[{"name":"bbhou","username":"bbhou","email":"1557740299@qq.com","commits":1,"url":"https://github.com/bbhou"}]},"readingTime":{"minutes":8.79,"words":2638},"filePathRelative":"posts/数据分析实战45讲/第二模块：数据分析算法篇/25丨KNN（下）：如何对手写数字进行识别？.md","localizedDate":"2025年8月16日","excerpt":"\\n<p><audio id=\\"audio\\" title=\\"25丨KNN（下）：如何对手写数字进行识别？\\" controls=\\"\\" preload=\\"none\\"><source id=\\"mp3\\" src=\\"https://static001.geekbang.org/resource/audio/bd/11/bd87b5d9179bfd3740fd19f6b7641b11.mp3\\"></audio></p>\\n<p>今天我来带你进行KNN的实战。上节课，我讲了KNN实际上是计算待分类物体与其他物体之间的距离，然后通过统计最近的K个邻居的分类情况，来决定这个物体的分类情况。</p>\\n<p>这节课，我们先看下如何在sklearn中使用KNN算法，然后通过sklearn中自带的手写数字数据集来进行实战。</p>","autoDesc":true}');export{c as comp,o as data};
